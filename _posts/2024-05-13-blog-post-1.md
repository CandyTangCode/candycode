---
title: Getting back on track! 
date: 2019-08-26
permalink: /posts/2019/08/blog-post-1/
tags:
  - Recent Updates  
  - Topics Reviews
---

Hi again and sorry for holding back for such a long time. I have been busy and sick for a few weekends. To be honest, open offices are a little too contagious. Finally, I am done with my Internship and I am back for my Ph.D.  

I want to go on to talk about the Double ML methods. This is a a work by Chernozhukov so I am going to just tell you that it is interesting. The heuristic comes from the the simple idea in linear regression. Suppose that 

$$y = X\beta + \epsilon $$

Then we have to find out the effect of $X_1$ on $y$. 

One method to do that is to find out the residue of $X_1$ against $X_2 \cdots X_p$, lets call it $r_{X_1}$ and the residue of $y$ against $X_2 \cdots X_p$, lets call it $r_y$. Then we do a linear regression of $r_y$ against $r_{X_1}$. Simple! In the case of linear regression, it is also true.

However, is it possible to replace all the linear regression with non-linear regressions? This paper says that sometimes we can and provide interesting proofs. 

Well, although not all method can be done this way, I do believe that this opens a door. We can use XGboost, GBM, Random Forest and other methods. It is all ideas that we can use. 

Reference
=======================================
Chernozhukov V, Chetverikov D, Demirer M, et al. Double machine learning for treatment and causal parameters[R]. Centre for Microdata Methods and Practice, Institute for Fiscal Studies, 2016.
